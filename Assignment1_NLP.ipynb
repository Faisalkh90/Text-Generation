{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### I assumed the following:\n",
        "- Tokenizes a corpus of text.\n",
        "- Generates n-gram models from 2-6.\n",
        "- Produce random phrases.\n",
        "- Find and save the 10 high frequent (trigram) in the corpus in a text file \n",
        "- The user enters desired sentence\n",
        "- The user enters the seed word\n",
        "- Print the sentence"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**1- Downloads the data required for tokenization. It also imports a number of modules for text processing and generation.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzSIDCf59SSL",
        "outputId": "f8b7d648-2e37-497a-d7da-4a365d5ceb29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2023.3.23)\n",
            "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install --user -U nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import random\n",
        "from nltk.tokenize import word_tokenize\n",
        "import collections"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**2- Load the corpus from KSUCCA Corpus Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "f = open(\"aa1.txt\", \"r\", encoding=\"utf-8\")\n",
        "corpus = f.read()\n",
        "f.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**3- Tokenize the corpus and generate N-gram models for n = 2 to 6**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokens = word_tokenize(corpus)\n",
        "\n",
        "ngram_models = {}\n",
        "for n in range(2, 7):\n",
        "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
        "    ngram_models[n] = list(ngrams)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**4- Get the 10 most frequent trigrams and save it into  frequent.txt**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "trigrams = ngram_models[3]\n",
        "freq_dist = collections.Counter(trigrams)\n",
        "\n",
        "common = freq_dist.most_common(10)\n",
        "\n",
        "f = open(\"frequent.txt\", \"w\", encoding=\"utf-8\")\n",
        "for trigram, frequency in common:\n",
        "    f.write(f\"Trigram: {trigram}: Frequency: {frequency}\\n\")\n",
        "f.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**5- Generate a sentence of num_words length starting with the start_word provided.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejH6estp5LNg",
        "outputId": "34cf9d7f-1a32-4741-b68f-a55572d3b1de"
      },
      "outputs": [],
      "source": [
        "# generate sentence function \n",
        "\n",
        "def generate_sentence(num_words, start_word):\n",
        "    generated_sentence = start_word\n",
        "    while len(generated_sentence.split()) < num_words:\n",
        "        if len(generated_sentence.split()) < 2:\n",
        "            possible_followers = [follower for (a, follower) in ngram_models[2] if a == generated_sentence.split()[-1]]\n",
        "        else:\n",
        "            possible_followers = [follower for (a, b, follower) in trigrams if a == generated_sentence.split()[-2] and b == generated_sentence.split()[-1]]\n",
        "            \n",
        "        if possible_followers:\n",
        "            generated_sentence += \" \" + random.choice(possible_followers)\n",
        "        else:\n",
        "            break\n",
        "    \n",
        "    return generated_sentence\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**6- Test and Result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test 1\n",
            " Number of words: 10 \n",
            " Start word: الله \n",
            " Sentence generated: الله على رسوله منهم فما أوجفتم عليه من أجر وما\n",
            "Test 2\n",
            " Number of words: 9 \n",
            " Start word: العزيز \n",
            " Sentence generated: العزيز الحميد الذي له ما في الأرض وجعلنا لكم\n",
            "Test 3\n",
            " Number of words: 8 \n",
            " Start word: الرحمن \n",
            " Sentence generated: الرحمن ولدا سبحانه بل عباد مكرمون لا يسبقونه\n",
            "Test 4\n",
            " Number of words: 7 \n",
            " Start word: أحد \n",
            " Sentence generated: أحد وامضوا حيث تؤمرون وقضينا إليه ذلك\n",
            "Test 5\n",
            " Number of words: 6 \n",
            " Start word: على \n",
            " Sentence generated: على غضب وللكافرين عذاب أليم ولله\n",
            "Test 6\n",
            " Number of words: 5 \n",
            " Start word: إن \n",
            " Sentence generated: إن الله غفور رحيم يا\n",
            "Test 7\n",
            " Number of words: 4 \n",
            " Start word: غفور \n",
            " Sentence generated: غفور رحيم فإذا قضيتم\n",
            "Test 8\n",
            " Number of words: 3 \n",
            " Start word: هو \n",
            " Sentence generated: هو خصيم مبين\n",
            "Test 9\n",
            " Number of words: 2 \n",
            " Start word: قريش \n",
            " Sentence generated: قريش إيلافهم\n",
            "Test 10\n",
            " Number of words: 1 \n",
            " Start word: الله \n",
            " Sentence generated: الله\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    num_words = int(input(\"Enter the number of words in the desired sentence: \"))\n",
        "    start_word = input(\"Enter a start word: \")\n",
        "    print(f\"Test {i+1}\\n Number of words: {num_words} \\n Start word: {start_word} \\n Sentence generated: {generate_sentence(num_words, start_word)}\")    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "**7-  Find and save the 10 high frequent (trigram) in the corpus in a text file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trigram: ('الله', 'الرحمن', 'الرحيم'): Frequency: 114\n",
            "Trigram: ('بسم', 'الله', 'الرحمن'): Frequency: 113\n",
            "Trigram: ('يا', 'أيها', 'الذين'): Frequency: 92\n",
            "Trigram: ('أيها', 'الذين', 'آمنوا'): Frequency: 89\n",
            "Trigram: ('من', 'دون', 'الله'): Frequency: 71\n",
            "Trigram: ('على', 'كل', 'شيء'): Frequency: 52\n",
            "Trigram: ('آمنوا', 'وعملوا', 'الصالحات'): Frequency: 50\n",
            "Trigram: ('إن', 'في', 'ذلك'): Frequency: 50\n",
            "Trigram: ('في', 'سبيل', 'الله'): Frequency: 44\n",
            "Trigram: ('ما', 'في', 'السماوات'): Frequency: 39\n"
          ]
        }
      ],
      "source": [
        "with open('frequent.txt', 'r' , encoding = 'utf8') as file:\n",
        "    for line in file:\n",
        "        print(line.strip())"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**8- Resources**"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- https://sourceforge.net/projects/ksucca-corpus/files/KSUCCA%20Files/\n",
        "\n",
        "- https://docs.python.org/3/library/random.html\n",
        "\n",
        "- https://www.digitalocean.com/community/tutorials/python-counter-python-collections-counter\n",
        "\n",
        "- https://github.com/shayan09/Text-Generation-using-NGRAM-models\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
