{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h3Co58u5myG"
      },
      "source": [
        "1- Use carpus to generte N-grams, where N between 2 to 5.\n",
        "\n",
        "2- The user should enter number of words (tokens) in the desired sentence.\n",
        "\n",
        "3- The output should have 10 words/tokens.\n",
        "\n",
        "4-  Evaluate 8 test samples by human expert (yourself or your\n",
        "friend). \n",
        "\n",
        "5- Submit:\n",
        "\n",
        "5.1 the code and the test sample (input + output)\n",
        "\n",
        "5.2 the 10 high frequent (trigram) in the corpus in a text file "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzSIDCf59SSL",
        "outputId": "f8b7d648-2e37-497a-d7da-4a365d5ceb29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (3.8.1)\n",
            "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (1.2.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (2023.3.23)\n",
            "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from nltk) (4.65.0)\n",
            "Requirement already satisfied: click in c:\\users\\user\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied: colorama in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (from tqdm->nltk) (0.4.6)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install --user -U nltk\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejH6estp5LNg",
        "outputId": "34cf9d7f-1a32-4741-b68f-a55572d3b1de"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "الله لا يضيع أجر المحسنين ولأجر الآخرة أكبر لو كانوا\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Load the corpus\n",
        "with open(\"aa1.txt\", \"r\", encoding=\"utf8\") as f:\n",
        "    corpus = f.read()\n",
        "\n",
        "# Tokenize the corpus\n",
        "tokens = word_tokenize(corpus)\n",
        "\n",
        "# Generate N-gram models for n = 2 to 6\n",
        "ngram_models = {}\n",
        "for n in range(2, 7):\n",
        "    ngram_models[n] = list(nltk.ngrams(tokens, n))\n",
        "\n",
        "# Get the frequency distribution of trigrams\n",
        "trigrams = ngram_models[3]\n",
        "freq_dist = nltk.FreqDist(trigrams)\n",
        "\n",
        "# Get the 10 most frequent trigrams\n",
        "common = freq_dist.most_common(10)\n",
        "\n",
        "# Save the most common trigrams to a text file\n",
        "with open(\"frequent.txt\", \"w\", encoding=\"utf8\") as f:\n",
        "    for trigram, frequency in common:\n",
        "        f.write(f\"{trigram}: {frequency}\\n\")\n",
        "\n",
        "# Generate sentences based on user input\n",
        "num_words = int(input(\"Enter the number of words in the desired sentence: \"))\n",
        "seed_word = input(\"Enter a seed word: \")\n",
        "\n",
        "# Use trigrams for generating the sentence\n",
        "generated_sentence = seed_word\n",
        "for i in range(num_words - 1):\n",
        "    if len(generated_sentence.split()) < 2:\n",
        "        # If the generated sentence has less than 2 words, use a bigram model instead\n",
        "        possible_followers = [follower for (a, follower) in ngram_models[2] if a == generated_sentence.split()[-1]]\n",
        "        if possible_followers:\n",
        "            generated_sentence += \" \" + random.choice(possible_followers)\n",
        "        else:\n",
        "            # If there are no possible followers in the bigram model, end the sentence\n",
        "            break\n",
        "    else:\n",
        "        possible_followers = [follower for (a, b, follower) in trigrams if a == generated_sentence.split()[-2] and b == generated_sentence.split()[-1]]\n",
        "        if possible_followers:\n",
        "            generated_sentence += \" \" + random.choice(possible_followers)\n",
        "        else:\n",
        "            # If there are no possible followers, use a bigram model instead\n",
        "            possible_followers = [follower for (a, follower) in ngram_models[2] if a == generated_sentence.split()[-1]]\n",
        "            if possible_followers:\n",
        "                generated_sentence += \" \" + random.choice(possible_followers)\n",
        "            else:\n",
        "                # If there are no possible followers in the bigram model, end the sentence\n",
        "                break\n",
        "\n",
        "\n",
        "        \n",
        "print(generated_sentence)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
